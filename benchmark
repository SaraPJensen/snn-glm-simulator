#!/bin/bash

set -e

mlnodes() {
    SERVER_NUMBER=6
    GPU_NR=3

    rsync -av --exclude .git --exclude __pycache__ . hermabr@ml8.hpc.uio.no:/itf-fi-ml/home/hermabr/jakob &> /dev/null

    ssh hermabr@ml${SERVER_NUMBER}.hpc.uio.no -t 'source .bashrc && \
        echo -e "\x1b[34mRunning \x1b[33mspiking network\x1b[34m on '$SERVER_NUMBER'\x1b[0m" && \
        module load PyTorch/1.11.0-foss-2021a-CUDA-11.3.1 && 
        cd jakob && \
        CUDA_VISIBLE_DEVICES='$GPU_NR' python -m benchmarking -n '$1' -t '$2' -N '$3' --data_path /scratch/users/hermabr/jakob/benchmarking/data'
}

simula() {
    rsync -av --exclude .git --exclude __pycache__ spiking_network ex3:~/

    ssh ex3 -t 'source .bashrc && \
        echo "Running on ex3" && \
        rm *.out | true && \
        /cm/shared/apps/slurm/20.02.7/bin/sbatch -p hgx2q time.sbatch '$1' '$2' '$3' && \
        until [ -f *.out ]
        do
             sleep 0.5
        done
        tail -f *.out'
}

if [ "$1" == "simula" ]; then
    simula $2 $3 $4
elif [ "$1" == "mlnodes" ]; then
    mlnodes $2 $3 $4
fi
set -e

rsync -av --exclude .git --exclude __pycache__ spiking_network ex3:~/

ssh ex3 -t 'source .bashrc && \
    echo "Running on ex3" && \
    rm *.out | true && \
    /cm/shared/apps/slurm/20.02.7/bin/sbatch -p hgx2q time.sbatch '$1' '$2' && \
    until [ -f *.out ]
    do
         sleep 0.5
    done
    tail -f *.out'
